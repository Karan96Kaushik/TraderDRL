{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c3a44a-1157-4237-90a5-4288d1ad1fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('t_multisite_electricity_price_202405261055.csv')\n",
    "\n",
    "df = df[df['distribution_id'] == 11]\n",
    "df['date_time'] = pd.to_datetime(df['date_time'])\n",
    "\n",
    "df['hour_of_day'] = df['date_time'].dt.hour + (df['date_time'].dt.minute > 0) * 0.5\n",
    "\n",
    "# df = df[(df['hour_of_day'] < 8) | (df['hour_of_day'] > 16.5)]\n",
    "\n",
    "df['hour_of_day'] = df['hour_of_day'].apply(lambda x: x - 12 if x >= 12 else x + 12)\n",
    "df['electricity_price'] = df['electricty_price_fixed']\n",
    "\n",
    "df = df.sort_values(by='date_time', ascending=True, ignore_index=True)\n",
    "\n",
    "pd.set_option('display.max_rows', 99)\n",
    "\n",
    "display(df.tail(99))\n",
    "\n",
    "print(df['electricty_price_fixed'].max())\n",
    "print(df['electricty_price_fixed'].min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2b1625-63e0-4a0c-9598-8d2ebf89652f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "class MultiAgentEVChargingEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, df, num_agents):\n",
    "        super(MultiAgentEVChargingEnv, self).__init__()\n",
    "        self.df = df\n",
    "\n",
    "        # if self.df_starters.empty:\n",
    "        #     raise ValueError(\"No data available for hours between 0 and 6.\")\n",
    "\n",
    "        self.num_agents = num_agents  # Number of vehicles\n",
    "\n",
    "        self.max_battery_level = 100.0 * 3\n",
    "        self.min_battery_level = 0\n",
    "        # self.deadline = 13.0  # hours until the deadline\n",
    "        self.target_soc = 90  # target state of charge\n",
    "        self.possible_charge_powers = [11, 22, 25]\n",
    "        \n",
    "        self.max_charge_power = max(self.possible_charge_powers)\n",
    "        self.min_charge_power = min(self.possible_charge_powers)\n",
    "        \n",
    "        self.max_battery_cap = 125\n",
    "        self.min_battery_cap = 55\n",
    "        \n",
    "        self.max_deadline_hours = 24\n",
    "        self.min_deadline_hours = 6\n",
    "\n",
    "        self.agents = {i: self.create_agent() for i in range(self.num_agents)}\n",
    "        \n",
    "        self.action_space = gym.spaces.Box(low=np.array([-1], dtype=np.float32), \n",
    "                                           high=np.array([1], dtype=np.float32), dtype=np.float32)\n",
    "        self.observation_space = gym.spaces.Box(low=-1, high=1, shape=(5,), dtype=np.float32)\n",
    "\n",
    "    \n",
    "    def rescale(self, val, max_val, min_val):\n",
    "        if min_val == max_val:\n",
    "            return 0\n",
    "        return 2 * (val - min_val) / (max_val - min_val) - 1\n",
    "\n",
    "    def rescale_to_original(self, rescaled_data, original_max, original_min):\n",
    "        return (rescaled_data + 1) * (original_max - original_min) / 2 + original_min\n",
    "\n",
    "    def create_agent(self):\n",
    "        \"\"\" Initialize a new agent with unique parameters. \"\"\"\n",
    "\n",
    "        agent = {}\n",
    "        agent['deadline_hours'] = float(random.randint(self.min_deadline_hours, self.max_deadline_hours))\n",
    "        agent['battery_level'] = random.randint(10, 60)\n",
    "        agent['battery_capacity'] = random.randint(self.min_battery_cap, self.max_battery_cap)\n",
    "        agent['max_draw'] = random.choice(self.possible_charge_powers)\n",
    "\n",
    "        # agent['deadline_hours'] = self.rescale(agent['deadline_hours'], self.max_deadline_hours, self.min_deadline_hours)\n",
    "        # agent['battery_level'] = self.rescale(agent['battery_level'], self.max_battery_level, self.min_battery_level)\n",
    "        # agent['battery_capacity'] = self.rescale(agent['battery_capacity'], self.max_battery_cap, self.min_battery_cap)\n",
    "        # agent['max_draw'] = self.rescale(agent['max_draw'], self.max_charge_power, self.min_charge_power)\n",
    "        \n",
    "        return agent\n",
    "\n",
    "    def setup_time_prices(self):\n",
    "        \n",
    "        self.current_index = random.choice(self.df.index.tolist())\n",
    "        price_size = ((self.max_deadline_hours*2) +5)\n",
    "        rows = self.df.iloc[self.current_index:self.current_index + price_size]\n",
    "\n",
    "        # self.hours = rows['hour_of_day'].values\n",
    "        self.prices = rows['electricity_price'].values\n",
    "        # print(len(self.prices), ((self.max_deadline_hours*2)+5))\n",
    "        if len(self.prices) < (self.max_deadline_hours*2):\n",
    "            return self.setup_time_prices()\n",
    "\n",
    "    def increment_time_prices(self):\n",
    "        # self.current_time = self.hours[0]\n",
    "        self.current_price = self.prices[0]\n",
    "        # self.hours = self.hours[1:]\n",
    "        self.prices = self.prices[1:]\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" Reset the state of each agent. \"\"\"\n",
    "        self.setup_time_prices()\n",
    "        self.increment_time_prices()\n",
    "        \n",
    "        states = {}\n",
    "        for agent_id in self.agents:\n",
    "            states[agent_id] = self.reset_agent(agent_id)\n",
    "        return states, {}\n",
    "\n",
    "    def reset_agent(self, agent_id):\n",
    "        \"\"\" Reset a single agent's state. \"\"\"\n",
    "        agent = self.agents[agent_id]\n",
    "\n",
    "        agent['deadline_hours'] = float(random.randint(self.min_deadline_hours, self.max_deadline_hours))\n",
    "        agent['battery_level'] = random.randint(10, 60)\n",
    "        agent['battery_capacity'] = random.randint(self.min_battery_cap, self.max_battery_cap)\n",
    "        agent['max_draw'] = random.choice(self.possible_charge_powers)\n",
    "\n",
    "        return self.calculate_observation(agent)\n",
    "\n",
    "    def calculate_observation(self, agent):\n",
    "        \n",
    "\n",
    "        deadline_hours = self.rescale(agent['deadline_hours'], self.max_deadline_hours, 0)\n",
    "        battery_level = self.rescale(agent['battery_level'], self.max_battery_level, self.min_battery_level)\n",
    "        battery_capacity = self.rescale(agent['battery_capacity'], self.max_battery_cap, self.min_battery_cap)\n",
    "        max_draw = self.rescale(agent['max_draw'], self.max_charge_power, self.min_charge_power)\n",
    "\n",
    "        print(deadline_hours, agent['deadline_hours'])\n",
    "        \n",
    "        return np.array(\n",
    "            [\n",
    "                float(battery_level), \n",
    "                float(deadline_hours), \n",
    "                float(self.current_price), \n",
    "                float(battery_capacity), \n",
    "                float(max_draw)\n",
    "            ], \n",
    "            dtype=np.float32)\n",
    "\n",
    "    def step(self, actions):\n",
    "        \"\"\" Apply actions, update states, and return observations and rewards for all agents. \"\"\"\n",
    "        next_states = {}\n",
    "        rewards = {}\n",
    "        dones = {}\n",
    "        truncateds = {'__all__': False}\n",
    "        infos = {}\n",
    "\n",
    "        self.increment_time_prices()\n",
    "        \n",
    "        for agent_id, action in actions.items():\n",
    "            next_states[agent_id], rewards[agent_id], dones[agent_id], truncateds[agent_id], infos[agent_id] = self.step_agent(agent_id, action)\n",
    "        # print(np.alltrue(dones.values()))\n",
    "        dones['__all__'] = np.alltrue(list(dones.values()))\n",
    "        # print(dones)\n",
    "        return next_states, rewards, dones, truncateds, infos\n",
    "\n",
    "    def step_agent(self, agent_id, action):\n",
    "        \"\"\" Update state for a single agent based on its action. \"\"\"\n",
    "        agent = self.agents[agent_id]\n",
    "\n",
    "        # charge_power = (+1)*(self.max_draw/2)\n",
    "\n",
    "        # agent['deadline_hours'] = self.rescale_to_original(agent['deadline_hours'], self.max_deadline_hours, self.min_deadline_hours)\n",
    "        # agent['battery_level'] = self.rescale_to_original(agent['battery_level'], self.max_battery_level, self.min_battery_level)\n",
    "        # agent['battery_capacity'] = self.rescale_to_original(agent['battery_capacity'], self.max_battery_cap, self.min_battery_cap)\n",
    "        # agent['max_draw'] = self.rescale_to_original(agent['max_draw'], self.max_charge_power, self.min_charge_power)\n",
    "\n",
    "        \n",
    "        charge_power = self.rescale_to_original(action, agent['max_draw'], 0)\n",
    "        reward = 0.0\n",
    "\n",
    "        if agent['deadline_hours'] > 0:\n",
    "            agent['deadline_hours'] -= 0.5\n",
    "\n",
    "            added_soc = (charge_power * 0.5 * 100) / agent['battery_capacity']\n",
    "            prev_battery_level = agent['battery_level']\n",
    "            agent['battery_level'] = agent['battery_level'] + added_soc\n",
    "\n",
    "            if agent['battery_level'] > self.max_battery_level:\n",
    "                agent['battery_level'] = self.max_battery_level\n",
    "            # agent['battery_level'] = min(self.max_battery_level, agent['battery_level'] + added_soc)\n",
    "\n",
    "            changed_soc = agent['battery_level'] - prev_battery_level \n",
    "\n",
    "            if abs(agent['battery_level'] - self.target_soc) < 5:\n",
    "                reward += 20  # Reward for reaching or exceeding target SOC\n",
    "\n",
    "            reward -= float(abs(self.target_soc - agent['battery_level']))\n",
    "\n",
    "            reward -= pow(float(charge_power * self.current_price * 3) / 2, 2)\n",
    "\n",
    "        done = bool(agent['deadline_hours'] <= 0)\n",
    "\n",
    "        # print(agent)\n",
    "\n",
    "        return self.calculate_observation(agent), reward, done, False, {}\n",
    "        \n",
    "    def render(self, mode='human'):\n",
    "        for agent_id, agent in self.agents.items():\n",
    "            print(f'Agent {agent_id}: Battery Level: {agent[\"battery_level\"]:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70018152",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MultiAgentEVChargingEnv(df, num_agents=5)\n",
    "\n",
    "env.reset()\n",
    "\n",
    "for _ in range(24):\n",
    "    a = env.step({0:0,1:0,2:0,3:0,4:0})\n",
    "    print(a[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7577f7cc-2ea8-4f70-89d6-3f4800de8a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.utils import conversions\n",
    "from pettingzoo.utils.env import ParallelEnv\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "# Assuming MultiAgentEVChargingEnv is correctly defined somewhere\n",
    "class MultiAgentEVChargingParaEnv(ParallelEnv):\n",
    "    metadata = {'render_modes': ['human'], 'name': \"MultiAgentEVCharging\"}\n",
    "\n",
    "    def __init__(self, df, num_agents):\n",
    "        super().__init__()\n",
    "        self.env = MultiAgentEVChargingEnv(df=df, num_agents=num_agents)\n",
    "        self.agents = ['agent_' + str(i) for i in range(num_agents)]\n",
    "        self.possible_agents = self.agents[:]\n",
    "        self.agent_name_mapping = dict(zip(self.possible_agents, list(range(len(self.possible_agents)))))\n",
    "\n",
    "        # Setup observation and action spaces\n",
    "        self.observation_space = gym.spaces.Box(low=-1, high=1, shape=(5,), dtype=np.float32)\n",
    "        self.action_space = gym.spaces.Box(low=np.array([-1], dtype=np.float32), high=np.array([1], dtype=np.float32), dtype=np.float32)\n",
    "\n",
    "    def observe(self, agent):\n",
    "        # Implement your observation here\n",
    "        return np.random.rand(5).astype(np.float32)  # Dummy implementation\n",
    "\n",
    "    def step(self, actions):\n",
    "        # Implement your step logic here\n",
    "        obs = {agent: np.random.rand(5).astype(np.float32) for agent in self.agents}  # Dummy implementation\n",
    "        rewards = {agent: np.random.rand() for agent in self.agents}\n",
    "        dones = {agent: False for agent in self.agents}\n",
    "        infos = {agent: {} for agent in self.agents}\n",
    "        return obs, rewards, dones, infos\n",
    "\n",
    "    def reset(self):\n",
    "        return {agent: np.random.rand(5).astype(np.float32) for agent in self.agents}  # Dummy implementation\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "# Import necessary libraries from Stable Baselines3 and PettingZoo\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from pettingzoo.utils.conversions import parallel_to_gym_wrapper\n",
    "\n",
    "# Create your environment and wrap it\n",
    "env = MultiAgentEVChargingParaEnv(df=\"your_dataframe\", num_agents=5)\n",
    "gym_env = parallel_to_gym_wrapper(env)  # Wrapping to Gym environment\n",
    "\n",
    "# Vectorize the environment\n",
    "vec_env = DummyVecEnv([lambda: gym_env])\n",
    "\n",
    "# Create and train the PPO model\n",
    "model = PPO(\"MlpPolicy\", vec_env, verbose=1)\n",
    "model.learn(total_timesteps=20000)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_multiagent_ev_charging\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acfa9ad-2f90-4ec5-a5c7-beae06897328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import ray\n",
    "from ray.rllib.algorithms import ppo\n",
    "from ray.rllib.env import MultiAgentEnv\n",
    "\n",
    "class WrappedMultiAgentEVChargingEnv(MultiAgentEnv):\n",
    "    def __init__(self, env_config):\n",
    "        self.env = MultiAgentEVChargingEnv(df=pd.DataFrame(env_config['df']), num_agents=env_config['num_agents'])\n",
    "        self.observation_space = self.env.observation_space  # Assuming the env defines this\n",
    "        self.action_space = self.env.action_space  # Assuming the env defines this\n",
    "\n",
    "    def reset(self, seed, options):\n",
    "        return self.env.reset()\n",
    "    \n",
    "    def step(self, action_dict):\n",
    "        return self.env.step(action_dict)\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        return self.env.render(mode)\n",
    "\n",
    "# ray.init()\n",
    "\n",
    "config = {\n",
    "    \"env\": \"multi_agent_ev_charging\",\n",
    "    \"env_config\": {\n",
    "        \"df\": df, \n",
    "        \"num_agents\": 5\n",
    "    },\n",
    "    \"num_workers\": 1,  # Parallelism\n",
    "    \"framework\": \"torch\",\n",
    "}\n",
    "\n",
    "algo = ppo.PPO(env=WrappedMultiAgentEVChargingEnv, config=config)\n",
    "\n",
    "# for i in range(5000):  # Number of training iterations\n",
    "#     result = algo.train()\n",
    "#     # print(result)\n",
    "#     print(f\"Iteration {i}\")\n",
    "#     print(f\"Mean Rew: {result['env_runners']['episode_reward_mean']}\")\n",
    "#     print(f\"Max Rew: {result['env_runners']['episode_reward_max']}\")\n",
    "#     print(f\"Min Rew: {result['env_runners']['episode_reward_min']}\")\n",
    "#     print(f\"Len: {result['env_runners']['episode_len_mean']}\")\n",
    "#     print('-----')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "56c3482e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Mean Rew: -9274.77702978719\n",
      "Max Rew: -2569.263101229062\n",
      "Min Rew: -19828.432112211616\n",
      "Len: 42.68327402135231\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):  # Number of training iterations\n",
    "    result = algo.train()\n",
    "    # print(result)\n",
    "    print(f\"Iteration {i}\")\n",
    "    print(f\"Mean Rew: {result['env_runners']['episode_reward_mean']}\")\n",
    "    print(f\"Max Rew: {result['env_runners']['episode_reward_max']}\")\n",
    "    print(f\"Min Rew: {result['env_runners']['episode_reward_min']}\")\n",
    "    print(f\"Len: {result['env_runners']['episode_len_mean']}\")\n",
    "    print('-----')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb6c707-0998-4af2-9bfa-d3f13a2b69ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.algorithms.ppo import PPO\n",
    "from ray.rllib.env import MultiAgentEnv\n",
    "from ray.tune.registry import register_env\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming MultiAgentEVChargingEnv is already imported and available\n",
    "class WrappedMultiAgentEVChargingEnv(MultiAgentEnv):\n",
    "    def __init__(self, env_config):\n",
    "        self.env = MultiAgentEVChargingEnv(df=pd.DataFrame(env_config['df']), num_agents=env_config['num_agents'])\n",
    "        self.observation_space = self.env.observation_space  # Assuming the env defines this\n",
    "        self.action_space = self.env.action_space  # Assuming the env defines this\n",
    "\n",
    "    def reset(self, seed=0, options={}):\n",
    "\n",
    "\n",
    "        return self.env.reset()\n",
    "    \n",
    "    def step(self, action_dict):\n",
    "        return self.env.step(action_dict)\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        return self.env.render(mode)\n",
    "\n",
    "# ray.init()\n",
    "\n",
    "# Register the environment\n",
    "register_env(\"multi_agent_ev_charging\", lambda config: WrappedMultiAgentEVChargingEnv(config))\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
    "    pol_id = 'shared_policy'\n",
    "    return pol_id\n",
    "\n",
    "# Configuration for Multi Actor Single Critic using PPO\n",
    "config = {\n",
    "    \"env\": \"multi_agent_ev_charging\",\n",
    "    \"env_config\": {\n",
    "        \"df\": df, \n",
    "        \"num_agents\": 5\n",
    "    },\n",
    "    \"multiagent\": {\n",
    "        \"policies\": {\n",
    "            \"shared_policy\": (None, None, None, {}),\n",
    "        },\n",
    "        \"policy_mapping_fn\": policy_mapping_fn,\n",
    "    },\n",
    "    \"num_workers\": 1,  # Parallelism\n",
    "    \"framework\": \"torch\",\n",
    "}\n",
    "\n",
    "\n",
    "# Create the PPO trainer with the configuration\n",
    "ppo_trainer = PPO(config=config)\n",
    "\n",
    "# Train the model\n",
    "for i in range(5000):  # Number of training iterations\n",
    "    result = ppo_trainer.train()\n",
    "    print(f\"Iteration {i}: {result['metrics']['episode_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfedd285",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c318e4-cf06-4b4d-98ff-351f093bf731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown Ray\n",
    "ray.shutdown()\n",
    "ray.init()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
